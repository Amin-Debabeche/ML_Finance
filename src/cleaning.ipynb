{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import wrds\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 7\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# note: wsb=wallstreetbets\n",
    "wsb_data_path = os.path.join(DATA_DIR, 'wsb_comments/wsb_comments_raw.csv')\n",
    "stock_data_path = os.path.join(DATA_DIR, 'GME')\n",
    "\n",
    "db = wrds.Connection(wrds_username='debabech', wrds_passeword='Electro1004$')\n",
    "db.create_pgpass_file()\n",
    "permcos = db.get_table(library='crsp', table='stocknames')[\n",
    "    [\"permco\", \"ticker\"]]\n",
    "\n",
    "# efficiency and accuracy --> \"en_core_web_trf\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsb_data(data_path, nrows=None):\n",
    "    \"Load wsb data, nrows None indicates all rows, otherwise specified integer of rows\"\n",
    "    return pd.read_csv(wsb_data_path, nrows=nrows, delimiter=',')\n",
    "\n",
    "\n",
    "def load_stock_wrds(ticker):\n",
    "    permco = permcos[permcos.ticker == ticker].permco.values[0]\n",
    "    # to 2021-03-25 ??\n",
    "    req = f\"select prc, date from crsp.dsf where permco in ({permco}) and date >='2012-01-31' and date <='2021-02-16'\"\n",
    "    asset = db.raw_sql(req, date_cols=['date'])\n",
    "    asset = asset.dropna()\n",
    "    asset[\"log_ret\"] = np.log(asset.prc).diff(1)\n",
    "\n",
    "    return asset[\"log_ret\"]\n",
    "\n",
    "\n",
    "def load_stock_yh(ticker):\n",
    "    asset = yf.download(ticker, interval='1d',\n",
    "                        start=\"2012-01-31\", end=\"2021-02-16\")\n",
    "    asset = asset.dropna()\n",
    "    asset[\"log_ret\"] = np.log(asset.Close - asset.Open) #np.log(asset.Open).diff(1).shift(-1)\n",
    "\n",
    "    return asset[\"log_ret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df = load_wsb_data(wsb_data_path, nrows=10_000)\n",
    "wsb_df['raw'] = wsb_df['body']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "SPY = load_stock_yh('SPY')\n",
    "DJI = load_stock_yh('DJI')\n",
    "NDAQ = load_stock_yh('NDAQ')\n",
    "INX = load_stock_yh('INX')\n",
    "NDXE = load_stock_yh('^NDXE')\n",
    "OEX = load_stock_yh('OEX')\n",
    "\n",
    "df_tickers = pd.concat([SPY, DJI, NDAQ, INX, NDXE, OEX], axis=1)\n",
    "df_tickers.columns = ['SPY', 'DJI', 'NDAQ', 'INX', 'NDXE', 'OEX']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreProcessing:\n",
    "\n",
    "    def __init__(self, wsb_data, lemmatize=True, lower_case=True, rem_stopwords=True, rem_punctuation=True, tokenize=True):\n",
    "        \"\"\"\n",
    "        Initialise all class parameters\n",
    "\n",
    "        :param data: nonempty pandas dataframe, wsb dataframe \n",
    "        :param lemmatize: bool, whether to perform lemmatization\n",
    "        :param lower_case: bool, whether to lowercase\n",
    "        :param rem_stopwords: bool, whether to remove stopwords\n",
    "        :param tokenize: bool, whether to tokenize\n",
    "        \"\"\"\n",
    "\n",
    "        self.wsb_data = wsb_data\n",
    "        self.lemmatize = lemmatize\n",
    "        self.lower_case = lower_case\n",
    "        self.rem_stopwords = rem_stopwords\n",
    "        self.rem_punctuation = rem_punctuation\n",
    "        self.tokenize = tokenize\n",
    "\n",
    "    # Ensure Parameter types\n",
    "    # K: need to add to this one gradually as we add columns that we use etc.\n",
    "    @property\n",
    "    def wsb_data(self):\n",
    "        return self._wsb_data\n",
    "\n",
    "    @wsb_data.setter\n",
    "    def wsb_data(self, wsb_data):\n",
    "\n",
    "        req_columns = ['author', 'body', 'created_utc']\n",
    "        str_columns = ['body']\n",
    "        date_columns = ['created_utc']\n",
    "\n",
    "        # Ensure the provided object is a dataframe\n",
    "        if not isinstance(wsb_data, pd.DataFrame):\n",
    "            raise Exception(\"The provided data must be a pandas Dataframe\")\n",
    "\n",
    "        # Ensure wsb dataframe is non empty\n",
    "        if wsb_data.shape[0] == 0:\n",
    "            raise Exception(\"Provided Dataframe is empty\")\n",
    "\n",
    "        # Ensure all required columns are provided\n",
    "        missing_columns = set(req_columns).difference(\n",
    "            set(wsb_data.columns.tolist()))\n",
    "        if len(missing_columns) > 0:\n",
    "            raise Exception(\n",
    "                f\"The columns {missing_columns} are missing from the provided dataframe!\")\n",
    "\n",
    "        # Ensure all column names don't have unexpected periods\n",
    "        if '.' in list(''.join(wsb_data.columns.tolist())):\n",
    "            raise Exception(\"All Column names must not include periods :'.'\")\n",
    "\n",
    "        # Ensure all string columns are strings\n",
    "        non_str_columns = set(str_columns).difference(\n",
    "            set(wsb_data.select_dtypes(include='object')))\n",
    "        if len(non_str_columns) > 0:\n",
    "            raise Exception(\n",
    "                f'The columns {non_str_columns} are expected as string (pandas object) columns.')\n",
    "\n",
    "        # Ensure dates are interpretable\n",
    "        for date_col in date_columns:\n",
    "            pd.to_datetime(wsb_data[date_col], unit='s', errors='ignore')\n",
    "            if pd.to_datetime(wsb_data[date_col], unit='s', errors='ignore').notnull().all():\n",
    "                try:\n",
    "                    # Otherwise Convert date using unixtimestamp to datetime object\n",
    "                    wsb_data[date_col] = pd.to_datetime(\n",
    "                        wsb_data[date_col], unit='s', errors='ignore')\n",
    "                except:\n",
    "                    raise Exception(\n",
    "                        f\"{date_col} must be a valid unixtimestamp format\")\n",
    "\n",
    "        self._wsb_data = wsb_data\n",
    "\n",
    "    @property\n",
    "    def lemmatize(self):\n",
    "        return self._lemmatize\n",
    "\n",
    "    @lemmatize.setter\n",
    "    def lemmatize(self, lemmatize):\n",
    "        if not isinstance(lemmatize, bool):\n",
    "            raise Exception(\n",
    "                'lemmatize must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._lemmatize = lemmatize\n",
    "\n",
    "    @property\n",
    "    def lower_case(self):\n",
    "        return self._lower_case\n",
    "\n",
    "    @lower_case.setter\n",
    "    def lower_case(self, lower_case):\n",
    "        if not isinstance(lower_case, bool):\n",
    "            raise Exception(\n",
    "                'lower_case must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._lower_case = lower_case\n",
    "\n",
    "    @property\n",
    "    def rem_stopwords(self):\n",
    "        return self._rem_stopwords\n",
    "\n",
    "    @rem_stopwords.setter\n",
    "    def rem_stopwords(self, rem_stopwords):\n",
    "        if not isinstance(rem_stopwords, bool):\n",
    "            raise Exception(\n",
    "                'rem_stopwords must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._rem_stopwords = rem_stopwords\n",
    "\n",
    "    @property\n",
    "    def rem_punctuation(self):\n",
    "        return self._rem_punctuation\n",
    "\n",
    "    @rem_punctuation.setter\n",
    "    def rem_punctuation(self, rem_punctuation):\n",
    "        if not isinstance(rem_punctuation, bool):\n",
    "            raise Exception(\n",
    "                'rem_punctuation must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._rem_punctuation = rem_punctuation\n",
    "\n",
    "    @property\n",
    "    def tokenize(self):\n",
    "        return self._tokenize\n",
    "\n",
    "    @tokenize.setter\n",
    "    def tokenize(self, tokenize):\n",
    "        if not isinstance(tokenize, bool):\n",
    "            raise Exception(\n",
    "                'tokenize must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._tokenize = tokenize\n",
    "\n",
    "    def clean_textual_data(self, textual_columns):\n",
    "\n",
    "        # Ensure the provided textual columns exist, and if single string column name convert it into a list\n",
    "        if len(textual_columns) < 1:\n",
    "            raise Exception(\n",
    "                'The number of textual columns to clean must be greater than 0')\n",
    "        if isinstance(textual_columns, str):\n",
    "            textual_columns = [textual_columns]\n",
    "        missing_columns = set(textual_columns).difference(\n",
    "            set(self.wsb_data.columns.tolist()))\n",
    "        if len(missing_columns) > 0:\n",
    "            raise Exception(\n",
    "                f\"The columns {missing_columns} to clean are missing from the wsb dataframe!\")\n",
    "\n",
    "        def lower_case_fn(self, col_name):\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].str.lower()\n",
    "            return self.wsb_data\n",
    "\n",
    "        def lemmatize_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def stemming_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            stemmer = nltk.stem.porter.PorterStemmer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: [stemmer.stem(w) for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def tokenize_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: [w for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def rem_punctuation_fn(self, col_name):\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: [w for w in x if w.isalnum()])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def rem_stopwords_fn(self, col_name):\n",
    "            \"stopwords dictionary considered English, wsb is an english forum\"\n",
    "            remove_elements = set(nltk.corpus.stopwords.words('english'))\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: [w for w in x if not w in remove_elements])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def remove_tokenization(self, col_name):\n",
    "            \"Necessary as final step to untokenize in case desired, tokenization required for other functions to not break\"\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(\n",
    "                lambda x: ' '.join(x))\n",
    "            return self.wsb_data\n",
    "\n",
    "        for textual_col in textual_columns:\n",
    "\n",
    "            if self.lower_case:\n",
    "                lower_case_fn(self, textual_col)\n",
    "\n",
    "            # lemmatize tokens if true, if false, stem tokens, if None then just tokenize\n",
    "            if self.lemmatize:\n",
    "                lemmatize_fn(self, textual_col)\n",
    "            elif self.lemmatize:\n",
    "                stemming_fn(self, textual_col)\n",
    "            else:\n",
    "                tokenize_fn(self, textual_col)\n",
    "\n",
    "            if self.rem_punctuation:\n",
    "                rem_punctuation_fn(self, textual_col)\n",
    "            if self.rem_stopwords:\n",
    "                rem_stopwords_fn(self, textual_col)\n",
    "            if not self.tokenize:\n",
    "                remove_tokenization(self, textual_col)\n",
    "\n",
    "        return self.wsb_data\n",
    "\n",
    "    # to later remove: for development\n",
    "\n",
    "    def output_data(self):\n",
    "        return self.wsb_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSBPreProcessing = PreProcessing(\n",
    "    wsb_df, lemmatize=True, lower_case=True, rem_stopwords=True, rem_punctuation=True, tokenize=True)\n",
    "WSBPreProcessing.clean_textual_data('body')\n",
    "useful_columns = ['author', 'raw', 'body',\n",
    "                  'created_utc', 'score', 'link_id', 'is_submitter']\n",
    "WSB_preprocessed_data = WSBPreProcessing.output_data()[useful_columns]\n",
    "WSB_preprocessed_data = WSB_preprocessed_data[WSB_preprocessed_data['author'] != '[deleted]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_url_at(df):\n",
    "    raw = []\n",
    "    for sentence in df['raw']:\n",
    "        sentence = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                          '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', sentence)\n",
    "        sentence = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", sentence)\n",
    "        raw.append(sentence)\n",
    "    df['raw'] = raw\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tagging(df):\n",
    "    pos,tag,dep,shape = [],[],[],[]\n",
    "    for sentence in df['raw']:\n",
    "        pos_tmp,tag_tmp,dep_tmp,shape_tmp = [],[],[],[]\n",
    "        for token in nlp(sentence):\n",
    "            pos_tmp.append(token.pos_)\n",
    "            tag_tmp.append(token.tag_)\n",
    "            dep_tmp.append(token.dep_)\n",
    "            shape_tmp.append(token.shape_)\n",
    "        pos.append(pos_tmp)\n",
    "        tag.append(tag_tmp)\n",
    "        dep.append(dep_tmp)\n",
    "        shape.append(shape_tmp)\n",
    "\n",
    "    df['Pos'] = pos\n",
    "    df['Tag'] = tag\n",
    "    df['Dep'] = dep\n",
    "    df['Shape'] = shape\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSB_preprocessed_data = rem_url_at(WSB_preprocessed_data)\n",
    "WSB_preprocessed_data = tagging(WSB_preprocessed_data)\n",
    "\n",
    "WSB_preprocessed_data.replace('', np.nan, inplace=True)\n",
    "WSB_preprocessed_data = WSB_preprocessed_data.dropna(how='any', axis=0)\n",
    "WSB_preprocessed_data = WSB_preprocessed_data[WSB_preprocessed_data['body'].map(\n",
    "    lambda d: len(d)) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyser\n",
    "- Flair\n",
    "- Vader\n",
    "- Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-31 18:58:56,021 loading file /Users/ade/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('en-sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser(df, v=True, f=True, b=True, r_vader=0.8, r_fler=0.1, r_blob=0.1):\n",
    "\n",
    "    df_fler, df_vader, df_blob = [], [], []\n",
    "    for sentence in tqdm(df['raw']):\n",
    "        if v:\n",
    "            df_vader.append(vader(sentence))\n",
    "        if f:\n",
    "            df_fler.append(fler(sentence))\n",
    "        if b:\n",
    "            df_blob.append(blob(sentence))\n",
    "    df['VADER'] = df_vader\n",
    "    df['FLAIR'] = df_fler\n",
    "    df['BLOB'] = df_blob\n",
    "\n",
    "    if v and f and b:\n",
    "        df['compound'] = df['VADER']*r_vader + \\\n",
    "            df['FLAIR']*r_fler + df['BLOB']*r_blob\n",
    "    elif v and f and not b:\n",
    "        df['compound'] = df['VADER']*(r_vader+r_blob) + df['FLAIR']*r_fler\n",
    "    elif v and b and not f:\n",
    "        df['compound'] = df['VADER']*(r_vader+r_fler) + df['BLOB']*r_blob\n",
    "    else:\n",
    "        df['compound'] = df['VADER']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def vader(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(sentence)['compound']\n",
    "    return float(sentiment)\n",
    "\n",
    "\n",
    "def fler(sentence):\n",
    "    s = Sentence(sentence)\n",
    "    classifier.predict(s)\n",
    "    sentiment = str(s.labels[0])\n",
    "    num = float(re.findall(r'\\d+\\.\\d+', sentiment)[0])\n",
    "    if sentiment.find('POSITIVE') == -1:\n",
    "        num = num * -1\n",
    "    return num\n",
    "\n",
    "\n",
    "def blob(sentence):\n",
    "    sentiment = TextBlob(sentence).sentiment.polarity\n",
    "    return sentiment\n",
    "\n",
    "\n",
    "WSB_preprocessed_data = sentiment_analyser(WSB_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Fourier Transform on Sentiment Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "\n",
    "def fourier(df, n_dimensions):\n",
    "    for n in n_dimensions:\n",
    "        n = round(n)\n",
    "        tmp_ = fft(df['compound'].values)\n",
    "        tmp_[n:-n] = 0\n",
    "        df['fourier_'+str(n)] = ifft(tmp_)\n",
    "    return df\n",
    "\n",
    "\n",
    "l = len(WSB_preprocessed_data)\n",
    "WSB_preprocessed_data = fourier(\n",
    "    WSB_preprocessed_data, np.linspace(round(l*0.1), round(l*0.9), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_df(df, df_tickers, column='compound', freq='D', var=1.1):\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    df['final_compound'] = df.score**var * df[column]\n",
    "    final_df['final_compound'] = df.groupby(\n",
    "        df['created_utc'].dt.to_period(freq)).final_compound.mean()\n",
    "    final_df.index = final_df.index.to_timestamp()\n",
    "    final_df = pd.merge(final_df, df_tickers, left_index=True, right_index=True)     # .dropna(subset=[\"final_compound\"], inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "final_df = merged_df(WSB_preprocessed_data, df_tickers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 8., 0., 2., 0., 1., 1., 0., 2., 1.]),\n",
       " array([0.        , 0.01714286, 0.03428571, 0.05142857, 0.06857143,\n",
       "        0.08571429, 0.10285714, 0.12      , 0.13714286, 0.15428571,\n",
       "        0.17142857]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=0.8, n_splits=3, test_size=0.1)\n",
    "\n",
    "X = final_df.final_compound\n",
    "y = final_df.SPY #for SPY\n",
    "# plt.hist(y.value_counts(normalize=True, bins=20))\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
