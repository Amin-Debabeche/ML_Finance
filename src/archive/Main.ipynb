{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4F Semester Project\n",
    "\n",
    "Current blocks/issues:\n",
    "- write here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>link_id</th>\n",
       "      <th>score</th>\n",
       "      <th>raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LazyMeal</td>\n",
       "      <td>We’re retarded and claim to be often. If you l...</td>\n",
       "      <td>1585123910</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fom9g6</td>\n",
       "      <td>1</td>\n",
       "      <td>We’re retarded and claim to be often. If you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math_salts</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1585123909</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legendary_Squirrel</td>\n",
       "      <td>markets been open for 13 min...</td>\n",
       "      <td>1585123905</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>1</td>\n",
       "      <td>markets been open for 13 min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSBMORONICTRADER</td>\n",
       "      <td>Spy can fuck around all it wants just as long ...</td>\n",
       "      <td>1585123901</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>1</td>\n",
       "      <td>Spy can fuck around all it wants just as long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1585123897</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fom0hg</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               body  \\\n",
       "0            LazyMeal  We’re retarded and claim to be often. If you l...   \n",
       "1          math_salts                                                Yes   \n",
       "2  Legendary_Squirrel                    markets been open for 13 min...   \n",
       "3    WSBMORONICTRADER  Spy can fuck around all it wants just as long ...   \n",
       "4           [deleted]                                          [removed]   \n",
       "\n",
       "   created_utc  is_submitter    link_id  score  \\\n",
       "0   1585123910         False  t3_fom9g6      1   \n",
       "1   1585123909         False  t3_fod66b      1   \n",
       "2   1585123905         False  t3_fod66b      1   \n",
       "3   1585123901         False  t3_fod66b      1   \n",
       "4   1585123897         False  t3_fom0hg      1   \n",
       "\n",
       "                                                 raw  \n",
       "0  We’re retarded and claim to be often. If you l...  \n",
       "1                                                Yes  \n",
       "2                    markets been open for 13 min...  \n",
       "3  Spy can fuck around all it wants just as long ...  \n",
       "4                                          [removed]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re  \n",
    "import pandas as pd  \n",
    "from time import time \n",
    "import sys\n",
    "import numpy as np\n",
    "import spacy  \n",
    "import logging  \n",
    "import nltk\n",
    "from datetime import datetime\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')\n",
    "import multiprocessing\n",
    "\n",
    "RANDOM_SEED = 7\n",
    "DATA_DIR = \"../data/\"\n",
    "wsb_data_path = os.path.join(DATA_DIR, 'wsb_comments/wsb_comments_raw.csv') #note: wsb=wallstreetbets\n",
    "stock_data_path = os.path.join(DATA_DIR, 'GME')\n",
    "\n",
    "def load_wsb_data(data_path, nrows=None, cols=['author', 'body','created_utc', 'score', 'link_id', 'is_submitter']):\n",
    "    \"Load wsb data, nrows None indicates all rows, otherwise specified integer of rows\"\n",
    "    return pd.read_csv(wsb_data_path, nrows = nrows, delimiter=',', usecols=cols)\n",
    "\n",
    "wsb_df = load_wsb_data(wsb_data_path, nrows=1000)\n",
    "wsb_df['raw'] = wsb_df['body']\n",
    "display(wsb_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing: \n",
    "    \n",
    "    def __init__(self, wsb_data, lemmatize=True, lower_case=True, rem_stopwords=True, rem_punctuation=True, tokenize=True):\n",
    "        \"\"\"\n",
    "        Initialise all class parameters\n",
    "        \n",
    "        :param data: nonempty pandas dataframe, wsb dataframe \n",
    "        :param lemmatize: bool, whether to perform lemmatization\n",
    "        :param lower_case: bool, whether to lowercase\n",
    "        :param rem_stopwords: bool, whether to remove stopwords\n",
    "        :param tokenize: bool, whether to tokenize\n",
    "        \"\"\"\n",
    "        \n",
    "        self.wsb_data = wsb_data\n",
    "        self.lemmatize = lemmatize\n",
    "        self.lower_case = lower_case\n",
    "        self.rem_stopwords = rem_stopwords\n",
    "        self.rem_punctuation = rem_punctuation\n",
    "        self.tokenize = tokenize\n",
    "        \n",
    "    ### Ensure Parameter types \n",
    "    #K: need to add to this one gradually as we add columns that we use etc.\n",
    "    @property\n",
    "    def wsb_data(self):\n",
    "        return self._wsb_data\n",
    "    @wsb_data.setter\n",
    "    def wsb_data(self, wsb_data):\n",
    "        \n",
    "        req_columns = ['author','body','created_utc']\n",
    "        str_columns = ['body']\n",
    "        date_columns = ['created_utc']\n",
    "\n",
    "        # Ensure the provided object is a dataframe\n",
    "        if not isinstance(wsb_data, pd.DataFrame):\n",
    "            raise Exception(\"The provided data must be a pandas Dataframe\")\n",
    "        \n",
    "        # Ensure wsb dataframe is non empty\n",
    "        if wsb_data.shape[0] == 0: \n",
    "            raise Exception(\"Provided Dataframe is empty\")\n",
    "        \n",
    "        # Ensure all required columns are provided\n",
    "        missing_columns = set(req_columns).difference(set(wsb_data.columns.tolist()))\n",
    "        if len(missing_columns) > 0:\n",
    "            raise Exception(f\"The columns {missing_columns} are missing from the provided dataframe!\")\n",
    "            \n",
    "        # Ensure all column names don't have unexpected periods\n",
    "        if '.' in list(''.join(wsb_data.columns.tolist())):\n",
    "            raise Exception(\"All Column names must not include periods :'.'\")\n",
    "            \n",
    "        # Ensure all string columns are strings\n",
    "        non_str_columns = set(str_columns).difference(set(wsb_data.select_dtypes(include='object')))\n",
    "        if len(non_str_columns) > 0:\n",
    "            raise Exception(f'The columns {non_str_columns} are expected as string (pandas object) columns.')\n",
    "        \n",
    "        # Ensure dates are interpretable\n",
    "        for date_col in date_columns: \n",
    "            if wsb_data[date_col].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S')).notnull().all():\n",
    "                try:\n",
    "                    # Otherwise Convert date using unixtimestamp to datetime object\n",
    "                    wsb_data[date_col] = wsb_data[date_col].apply(lambda x: datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                except: \n",
    "                    raise Exception(f\"{date_col} must be a valid unixtimestamp format\")\n",
    "                    \n",
    "        self._wsb_data = wsb_data\n",
    "        \n",
    "    @property\n",
    "    def lemmatize(self):\n",
    "        return self._lemmatize\n",
    "    @lemmatize.setter\n",
    "    def lemmatize(self, lemmatize):\n",
    "        if not isinstance(lemmatize, bool): \n",
    "            raise Exception('lemmatize must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._lemmatize = lemmatize\n",
    "    \n",
    "    @property\n",
    "    def lower_case(self):\n",
    "        return self._lower_case\n",
    "    @lower_case.setter\n",
    "    def lower_case(self, lower_case):\n",
    "        if not isinstance(lower_case, bool): \n",
    "            raise Exception('lower_case must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._lower_case = lower_case\n",
    "      \n",
    "    @property\n",
    "    def rem_stopwords(self):\n",
    "        return self._rem_stopwords\n",
    "    @rem_stopwords.setter\n",
    "    def rem_stopwords(self, rem_stopwords):\n",
    "        if not isinstance(rem_stopwords, bool): \n",
    "            raise Exception('rem_stopwords must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._rem_stopwords = rem_stopwords\n",
    "        \n",
    "    @property\n",
    "    def rem_punctuation(self):\n",
    "        return self._rem_punctuation\n",
    "    @rem_punctuation.setter\n",
    "    def rem_punctuation(self, rem_punctuation):\n",
    "        if not isinstance(rem_punctuation, bool): \n",
    "            raise Exception('rem_punctuation must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._rem_punctuation = rem_punctuation\n",
    "        \n",
    "    @property\n",
    "    def tokenize(self):\n",
    "        return self._tokenize\n",
    "    @tokenize.setter\n",
    "    def tokenize(self, tokenize):\n",
    "        if not isinstance(tokenize, bool): \n",
    "            raise Exception('tokenize must be provided as a boolean parameter (True/False) to the class')\n",
    "        self._tokenize = tokenize\n",
    "        \n",
    "    def clean_textual_data(self, textual_columns):\n",
    "        \n",
    "        ### Ensure the provided textual columns exist, and if single string column name convert it into a list\n",
    "        if len(textual_columns)<1:\n",
    "            raise Exception('The number of textual columns to clean must be greater than 0')\n",
    "        if isinstance(textual_columns, str):\n",
    "            textual_columns = [textual_columns]\n",
    "        missing_columns = set(textual_columns).difference(set(self.wsb_data.columns.tolist()))\n",
    "        if len(missing_columns) > 0:\n",
    "            raise Exception(f\"The columns {missing_columns} to clean are missing from the wsb dataframe!\")\n",
    "\n",
    "        def lower_case_fn(self, col_name): \n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].str.lower()\n",
    "            return self.wsb_data\n",
    "\n",
    "        def lemmatize_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def stemming_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            stemmer = nltk.stem.porter.PorterStemmer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: [stemmer.stem(w) for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def tokenize_fn(self, col_name):\n",
    "            w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: [w for w in w_tokenizer.tokenize(x)])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def rem_punctuation_fn(self, col_name):\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: [w for w in x if w.isalnum()])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def rem_stopwords_fn(self, col_name):\n",
    "            \"stopwords dictionary considered English, wsb is an english forum\"\n",
    "            remove_elements = set(nltk.corpus.stopwords.words('english'))\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: [w for w in x if not w in remove_elements])\n",
    "            return self.wsb_data\n",
    "\n",
    "        def remove_tokenization(self, col_name):\n",
    "            \"Necessary as final step to untokenize in case desired, tokenization required for other functions to not break\"\n",
    "            self.wsb_data[col_name] = self.wsb_data[col_name].apply(lambda x: ' '.join(x))\n",
    "            return self.wsb_data\n",
    "\n",
    "        for textual_col in textual_columns:\n",
    "\n",
    "            if self.lower_case:\n",
    "                lower_case_fn(self, textual_col)\n",
    "\n",
    "            # lemmatize tokens if true, if false, stem tokens, if None then just tokenize\n",
    "            if self.lemmatize:\n",
    "                lemmatize_fn(self, textual_col)\n",
    "            elif self.lemmatize:\n",
    "                stemming_fn(self, textual_col)\n",
    "            else: \n",
    "                tokenize_fn(self, textual_col)\n",
    "\n",
    "            if self.rem_punctuation:\n",
    "                rem_punctuation_fn(self, textual_col)\n",
    "            if self.rem_stopwords:\n",
    "                rem_stopwords_fn(self, textual_col)\n",
    "            if not self.tokenize:\n",
    "                remove_tokenization(self, textual_col)\n",
    "\n",
    "        return self.wsb_data\n",
    "        \n",
    "        \n",
    "    # to later remove: for development\n",
    "    def output_data(self):\n",
    "        return self.wsb_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSBPreProcessing = PreProcessing(wsb_df, lemmatize=True, lower_case=True, rem_stopwords=True, rem_punctuation=True, tokenize=True)\n",
    "WSBPreProcessing.clean_textual_data('body')\n",
    "useful_columns = ['author','raw', 'body','created_utc', 'score', 'link_id', 'is_submitter']\n",
    "WSB_preprocessed_data = WSBPreProcessing.output_data()[useful_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- Get rid of deleted comments and row with empty or nan values\n",
    "- Add POS, Tag, Dep, Shape\n",
    "- Remove URL and @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kamran/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # efficiency and accuracy --> \"en_core_web_trf\"\n",
    "\n",
    "WSB_preprocessed_data = WSB_preprocessed_data[WSB_preprocessed_data['author'] != '[deleted]']\n",
    "\n",
    "raw = []\n",
    "pos = []\n",
    "tag = []\n",
    "dep = []\n",
    "shape = []\n",
    "\n",
    "for sentence in WSB_preprocessed_data['raw']:\n",
    "    sentence = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', sentence)\n",
    "    sentence = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", sentence)\n",
    "    raw.append(sentence)\n",
    "    \n",
    "    pos_tmp = []\n",
    "    tag_tmp = []\n",
    "    dep_tmp = []\n",
    "    shape_tmp = []\n",
    "    for token in nlp(sentence):\n",
    "        pos_tmp.append(token.pos_)\n",
    "        tag_tmp.append(token.tag_)\n",
    "        dep_tmp.append(token.dep_)\n",
    "        shape_tmp.append(token.shape_)\n",
    "    pos.append(pos_tmp)\n",
    "    tag.append(tag_tmp)\n",
    "    dep.append(dep_tmp)\n",
    "    shape.append(shape_tmp) \n",
    "     \n",
    "WSB_preprocessed_data['Pos'] = pos\n",
    "WSB_preprocessed_data['Tag'] = tag\n",
    "WSB_preprocessed_data['Dep'] = dep\n",
    "WSB_preprocessed_data['Shape'] = shape    \n",
    "\n",
    "WSB_preprocessed_data['raw'] = raw\n",
    "\n",
    "WSB_preprocessed_data.replace('', np.nan, inplace=True)\n",
    "WSB_preprocessed_data = WSB_preprocessed_data.dropna(how='any',axis=0)\n",
    "WSB_preprocessed_data = WSB_preprocessed_data[WSB_preprocessed_data['body'].map(lambda d: len(d)) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyser\n",
    "- Flair\n",
    "- Vader\n",
    "- Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 10:32:54,452 loading file /home/kamran/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fler = []\n",
    "vader = []\n",
    "blob = []\n",
    "\n",
    "for i in range(len(WSB_preprocessed_data)):\n",
    "    raw = WSB_preprocessed_data['raw'].iloc[i]\n",
    "    sentence = WSB_preprocessed_data['body'].iloc[i]\n",
    "    \n",
    "    # FLAIR\n",
    "    s = Sentence(sentence)\n",
    "    classifier.predict(s)\n",
    "    total_sentiment = str(s.labels[0])\n",
    "    num = float(re.findall(r'\\d+\\.\\d+', total_sentiment)[0])\n",
    "    if total_sentiment.find('POSITIVE') == -1:\n",
    "        num = num * -1\n",
    "    fler.append(num)\n",
    "\n",
    "    # VADER\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(raw)['compound']\n",
    "    vader.append(float(vs))\n",
    "\n",
    "    # BLOB\n",
    "    _tmp = TextBlob(raw).sentiment.polarity\n",
    "    blob.append(float(_tmp))\n",
    "    \n",
    "WSB_preprocessed_data['FLAIR'] = fler\n",
    "WSB_preprocessed_data['VADER'] = vader\n",
    "WSB_preprocessed_data['BLOB'] = blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>raw</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>link_id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Shape</th>\n",
       "      <th>FLAIR</th>\n",
       "      <th>VADER</th>\n",
       "      <th>BLOB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LazyMeal</td>\n",
       "      <td>We’re retarded and claim to be often. If you l...</td>\n",
       "      <td>[retarded, claim, listen, doe, make]</td>\n",
       "      <td>2020-03-25 08:11:50</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fom9g6</td>\n",
       "      <td>False</td>\n",
       "      <td>[PRON, VERB, ADJ, CCONJ, VERB, PART, AUX, ADV,...</td>\n",
       "      <td>[PRP, VBZ, JJ, CC, VBP, TO, VB, RB, ., IN, PRP...</td>\n",
       "      <td>[nsubjpass, auxpass, ROOT, cc, conj, aux, xcom...</td>\n",
       "      <td>[Xx, ’xx, xxxx, xxx, xxxx, xx, xx, xxxx, ., Xx...</td>\n",
       "      <td>-0.9713</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math_salts</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[yes]</td>\n",
       "      <td>2020-03-25 08:11:49</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>False</td>\n",
       "      <td>[INTJ]</td>\n",
       "      <td>[UH]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[Xxx]</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legendary_Squirrel</td>\n",
       "      <td>markets been open for 13 min...</td>\n",
       "      <td>[market, open, 13]</td>\n",
       "      <td>2020-03-25 08:11:45</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>False</td>\n",
       "      <td>[NOUN, AUX, ADJ, ADP, NUM, NOUN, PUNCT]</td>\n",
       "      <td>[NNS, VBN, JJ, IN, CD, NN, .]</td>\n",
       "      <td>[nsubj, ROOT, acomp, prep, nummod, pobj, punct]</td>\n",
       "      <td>[xxxx, xxxx, xxxx, xxx, dd, xxx, ...]</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSBMORONICTRADER</td>\n",
       "      <td>Spy can fuck around all it wants just as long ...</td>\n",
       "      <td>[spy, fuck, around, want, long, 220, put, prin...</td>\n",
       "      <td>2020-03-25 08:11:41</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>False</td>\n",
       "      <td>[NOUN, VERB, VERB, ADV, DET, PRON, VERB, ADV, ...</td>\n",
       "      <td>[NN, MD, VB, RB, DT, PRP, VBZ, RB, RB, RB, IN,...</td>\n",
       "      <td>[nsubj, aux, ROOT, prep, pobj, nsubj, relcl, a...</td>\n",
       "      <td>[Xxx, xxx, xxxx, xxxx, xxx, xx, xxxx, xxxx, xx...</td>\n",
       "      <td>-0.9948</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>madamlazonga</td>\n",
       "      <td>you lost me at \"bulls fucked\"</td>\n",
       "      <td>[lost]</td>\n",
       "      <td>2020-03-25 08:11:36</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_fod66b</td>\n",
       "      <td>False</td>\n",
       "      <td>[PRON, VERB, PRON, ADP, PUNCT, NOUN, VERB, PUNCT]</td>\n",
       "      <td>[PRP, VBD, PRP, IN, ``, NNS, VBN, '']</td>\n",
       "      <td>[nsubj, ROOT, dobj, prep, punct, nsubj, pcomp,...</td>\n",
       "      <td>[xxx, xxxx, xx, xx, \", xxxx, xxxx, \"]</td>\n",
       "      <td>-0.9932</td>\n",
       "      <td>-0.7717</td>\n",
       "      <td>-0.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>plimsickins16978</td>\n",
       "      <td>ANAL 12/11 $50</td>\n",
       "      <td>[anal]</td>\n",
       "      <td>2020-12-05 20:08:38</td>\n",
       "      <td>4</td>\n",
       "      <td>t3_k6tl0d</td>\n",
       "      <td>False</td>\n",
       "      <td>[ADJ, NUM, SYM, NUM]</td>\n",
       "      <td>[JJ, CD, $, CD]</td>\n",
       "      <td>[compound, ROOT, nmod, npadvmod]</td>\n",
       "      <td>[XXXX, dd/dd, $, dd]</td>\n",
       "      <td>0.8674</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>PlentyC</td>\n",
       "      <td>Mmm fuck yeah</td>\n",
       "      <td>[mmm, fuck, yeah]</td>\n",
       "      <td>2020-12-05 20:08:37</td>\n",
       "      <td>7</td>\n",
       "      <td>t3_k6tl0d</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, ADJ, INTJ]</td>\n",
       "      <td>[NNP, JJ, UH]</td>\n",
       "      <td>[compound, ROOT, ROOT]</td>\n",
       "      <td>[Xxx, xxxx, xxxx]</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>-0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Jujubewise</td>\n",
       "      <td>Haha 😆</td>\n",
       "      <td>[haha]</td>\n",
       "      <td>2020-12-05 20:08:36</td>\n",
       "      <td>7</td>\n",
       "      <td>t3_k6tl0d</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[dep, ROOT]</td>\n",
       "      <td>[Xxxx, 😆]</td>\n",
       "      <td>-0.9691</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>steve_pops_01</td>\n",
       "      <td>Real</td>\n",
       "      <td>[real]</td>\n",
       "      <td>2020-12-05 20:08:33</td>\n",
       "      <td>5</td>\n",
       "      <td>t3_k6tl0d</td>\n",
       "      <td>False</td>\n",
       "      <td>[ADJ]</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[Xxxx]</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>k1tch3nMitts</td>\n",
       "      <td>lol</td>\n",
       "      <td>[lol]</td>\n",
       "      <td>2020-12-05 20:08:31</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_k7anqp</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN]</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[xxx]</td>\n",
       "      <td>0.7374</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                                raw  \\\n",
       "0              LazyMeal  We’re retarded and claim to be often. If you l...   \n",
       "1            math_salts                                                Yes   \n",
       "2    Legendary_Squirrel                    markets been open for 13 min...   \n",
       "3      WSBMORONICTRADER  Spy can fuck around all it wants just as long ...   \n",
       "5          madamlazonga                      you lost me at \"bulls fucked\"   \n",
       "..                  ...                                                ...   \n",
       "993    plimsickins16978                                     ANAL 12/11 $50   \n",
       "994             PlentyC                                      Mmm fuck yeah   \n",
       "995          Jujubewise                                             Haha 😆   \n",
       "998       steve_pops_01                                               Real   \n",
       "999        k1tch3nMitts                                                lol   \n",
       "\n",
       "                                                  body          created_utc  \\\n",
       "0                 [retarded, claim, listen, doe, make]  2020-03-25 08:11:50   \n",
       "1                                                [yes]  2020-03-25 08:11:49   \n",
       "2                                   [market, open, 13]  2020-03-25 08:11:45   \n",
       "3    [spy, fuck, around, want, long, 220, put, prin...  2020-03-25 08:11:41   \n",
       "5                                               [lost]  2020-03-25 08:11:36   \n",
       "..                                                 ...                  ...   \n",
       "993                                             [anal]  2020-12-05 20:08:38   \n",
       "994                                  [mmm, fuck, yeah]  2020-12-05 20:08:37   \n",
       "995                                             [haha]  2020-12-05 20:08:36   \n",
       "998                                             [real]  2020-12-05 20:08:33   \n",
       "999                                              [lol]  2020-12-05 20:08:31   \n",
       "\n",
       "     score    link_id  is_submitter  \\\n",
       "0        1  t3_fom9g6         False   \n",
       "1        1  t3_fod66b         False   \n",
       "2        1  t3_fod66b         False   \n",
       "3        1  t3_fod66b         False   \n",
       "5        1  t3_fod66b         False   \n",
       "..     ...        ...           ...   \n",
       "993      4  t3_k6tl0d         False   \n",
       "994      7  t3_k6tl0d         False   \n",
       "995      7  t3_k6tl0d         False   \n",
       "998      5  t3_k6tl0d         False   \n",
       "999      1  t3_k7anqp         False   \n",
       "\n",
       "                                                   Pos  \\\n",
       "0    [PRON, VERB, ADJ, CCONJ, VERB, PART, AUX, ADV,...   \n",
       "1                                               [INTJ]   \n",
       "2              [NOUN, AUX, ADJ, ADP, NUM, NOUN, PUNCT]   \n",
       "3    [NOUN, VERB, VERB, ADV, DET, PRON, VERB, ADV, ...   \n",
       "5    [PRON, VERB, PRON, ADP, PUNCT, NOUN, VERB, PUNCT]   \n",
       "..                                                 ...   \n",
       "993                               [ADJ, NUM, SYM, NUM]   \n",
       "994                                 [PROPN, ADJ, INTJ]   \n",
       "995                                     [PROPN, PROPN]   \n",
       "998                                              [ADJ]   \n",
       "999                                            [PROPN]   \n",
       "\n",
       "                                                   Tag  \\\n",
       "0    [PRP, VBZ, JJ, CC, VBP, TO, VB, RB, ., IN, PRP...   \n",
       "1                                                 [UH]   \n",
       "2                        [NNS, VBN, JJ, IN, CD, NN, .]   \n",
       "3    [NN, MD, VB, RB, DT, PRP, VBZ, RB, RB, RB, IN,...   \n",
       "5                [PRP, VBD, PRP, IN, ``, NNS, VBN, '']   \n",
       "..                                                 ...   \n",
       "993                                    [JJ, CD, $, CD]   \n",
       "994                                      [NNP, JJ, UH]   \n",
       "995                                         [NNP, NNP]   \n",
       "998                                               [JJ]   \n",
       "999                                              [NNP]   \n",
       "\n",
       "                                                   Dep  \\\n",
       "0    [nsubjpass, auxpass, ROOT, cc, conj, aux, xcom...   \n",
       "1                                               [ROOT]   \n",
       "2      [nsubj, ROOT, acomp, prep, nummod, pobj, punct]   \n",
       "3    [nsubj, aux, ROOT, prep, pobj, nsubj, relcl, a...   \n",
       "5    [nsubj, ROOT, dobj, prep, punct, nsubj, pcomp,...   \n",
       "..                                                 ...   \n",
       "993                   [compound, ROOT, nmod, npadvmod]   \n",
       "994                             [compound, ROOT, ROOT]   \n",
       "995                                        [dep, ROOT]   \n",
       "998                                             [ROOT]   \n",
       "999                                             [ROOT]   \n",
       "\n",
       "                                                 Shape   FLAIR   VADER    BLOB  \n",
       "0    [Xx, ’xx, xxxx, xxx, xxxx, xx, xx, xxxx, ., Xx... -0.9713 -0.5719 -0.8000  \n",
       "1                                                [Xxx]  0.9918  0.4019  0.0000  \n",
       "2                [xxxx, xxxx, xxxx, xxx, dd, xxx, ...]  0.8276  0.0000  0.0000  \n",
       "3    [Xxx, xxx, xxxx, xxxx, xxx, xx, xxxx, xxxx, xx... -0.9948 -0.5423 -0.0625  \n",
       "5                [xxx, xxxx, xx, xx, \", xxxx, xxxx, \"] -0.9932 -0.7717 -0.6000  \n",
       "..                                                 ...     ...     ...     ...  \n",
       "993                               [XXXX, dd/dd, $, dd]  0.8674  0.0000  0.0000  \n",
       "994                                  [Xxx, xxxx, xxxx]  0.9925 -0.3182 -0.4000  \n",
       "995                                          [Xxxx, 😆] -0.9691  0.4588  0.2000  \n",
       "998                                             [Xxxx]  0.9993  0.0000  0.2000  \n",
       "999                                              [xxx]  0.7374  0.4215  0.8000  \n",
       "\n",
       "[883 rows x 14 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WSB_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts Cluster\n",
    "- By Cluster\n",
    "- By Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "searchfor = ['aapl', 'apple', 'Apple', 'AAPL'] #add ISIN, Permco, etc..\n",
    "ticker_posts = WSB_preprocessed_data[WSB_preprocessed_data['raw'].str.contains('|'.join(searchfor))]\n",
    "ticker_posts.sort_values(by=['created_utc'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>raw</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>link_id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Shape</th>\n",
       "      <th>FLAIR</th>\n",
       "      <th>VADER</th>\n",
       "      <th>BLOB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>LegNest</td>\n",
       "      <td>oof aapl  **109.26**</td>\n",
       "      <td>[oof, aapl]</td>\n",
       "      <td>2020-10-30 08:06:39</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_jkhkw8</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, PROPN, SPACE, PUNCT, PUNCT, NUM, PUNCT...</td>\n",
       "      <td>[NNP, NNP, _SP, NFP, NFP, CD, NFP, NFP]</td>\n",
       "      <td>[amod, ROOT, , dep, dep, ROOT, dep, ROOT]</td>\n",
       "      <td>[xxx, xxxx,  , *, *, ddd.dd, *, *]</td>\n",
       "      <td>-0.9887</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Agent248</td>\n",
       "      <td>Holding apple calls is making me gay from last...</td>\n",
       "      <td>[holding, apple, call, making, gay, last, 2, day]</td>\n",
       "      <td>2020-09-23 20:11:29</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_iyhxsc</td>\n",
       "      <td>False</td>\n",
       "      <td>[VERB, NOUN, NOUN, AUX, VERB, PRON, ADJ, ADP, ...</td>\n",
       "      <td>[VBG, NN, NNS, VBZ, VBG, PRP, JJ, IN, JJ, CD, ...</td>\n",
       "      <td>[csubj, compound, dobj, aux, ROOT, nsubj, ccom...</td>\n",
       "      <td>[Xxxxx, xxxx, xxxx, xx, xxxx, xx, xxx, xxxx, x...</td>\n",
       "      <td>-0.9977</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>sandawg_</td>\n",
       "      <td>Hey guys! 😃 AAPL gang checking in 🙂. Today was...</td>\n",
       "      <td>[hey, aapl, gang, checking, today, wa, tough, ...</td>\n",
       "      <td>2020-09-23 20:11:05</td>\n",
       "      <td>-1</td>\n",
       "      <td>t3_iyhxsc</td>\n",
       "      <td>False</td>\n",
       "      <td>[INTJ, NOUN, PUNCT, PROPN, PROPN, NOUN, VERB, ...</td>\n",
       "      <td>[UH, NNS, ., NNP, NNP, NN, VBG, IN, NNP, ., NN...</td>\n",
       "      <td>[intj, ROOT, punct, compound, compound, nsubj,...</td>\n",
       "      <td>[Xxx, xxxx, !, 😃, XXXX, xxxx, xxxx, xx, 🙂, ., ...</td>\n",
       "      <td>-0.9709</td>\n",
       "      <td>0.6430</td>\n",
       "      <td>0.136508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Graxiano</td>\n",
       "      <td>AAPL is so last week, hype from split has died...</td>\n",
       "      <td>[aapl, last, hype, split, ha, died, tsla, new]</td>\n",
       "      <td>2020-08-18 08:06:48</td>\n",
       "      <td>3</td>\n",
       "      <td>t3_iblrnn</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, AUX, ADV, ADJ, NOUN, PUNCT, NOUN, ADP,...</td>\n",
       "      <td>[NNP, VBZ, RB, JJ, NN, ,, NN, IN, NN, VBZ, VBN...</td>\n",
       "      <td>[nsubj, ccomp, advmod, amod, npadvmod, punct, ...</td>\n",
       "      <td>[XXXX, xx, xx, xxxx, xxxx, ,, xxxx, xxxx, xxxx...</td>\n",
       "      <td>-0.9999</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>-0.006397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Throwawayeconboi</td>\n",
       "      <td>AAPL is as FLAT as a white woman’s ass. \\n\\n\\n...</td>\n",
       "      <td>[aapl, flat, white, fuuuuuuuuuuuuuuuuuu]</td>\n",
       "      <td>2020-08-18 08:03:50</td>\n",
       "      <td>6</td>\n",
       "      <td>t3_iblrnn</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, AUX, ADV, ADJ, SCONJ, DET, ADJ, NOUN, ...</td>\n",
       "      <td>[NNP, VBZ, RB, JJ, IN, DT, JJ, NN, POS, NN, .,...</td>\n",
       "      <td>[nsubj, ROOT, advmod, acomp, prep, det, amod, ...</td>\n",
       "      <td>[XXXX, xx, xx, XXXX, xx, x, xxxx, xxxx, ’x, xx...</td>\n",
       "      <td>-0.9963</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>strawberry-jam-boy</td>\n",
       "      <td>Spce tsla se lulu plug wmt upwk amd aapl msft ...</td>\n",
       "      <td>[spce, tsla, se, lulu, plug, wmt, upwk, amd, a...</td>\n",
       "      <td>2020-07-12 20:09:20</td>\n",
       "      <td>6</td>\n",
       "      <td>t3_hq11ao</td>\n",
       "      <td>False</td>\n",
       "      <td>[NOUN, NOUN, VERB, NOUN, PROPN, PROPN, PROPN, ...</td>\n",
       "      <td>[NN, NNS, VBP, NN, NNP, NNP, NNP, NNP, NNP, NN...</td>\n",
       "      <td>[compound, compound, compound, compound, compo...</td>\n",
       "      <td>[Xxxx, xxxx, xx, xxxx, xxxx, xxx, xxxx, xxx, x...</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>philmacrack123</td>\n",
       "      <td>Y'all see amazon, if apple is bad, we will sel...</td>\n",
       "      <td>[see, apple, gonna, need, appl, easily, carry,...</td>\n",
       "      <td>2020-04-30 20:11:50</td>\n",
       "      <td>6</td>\n",
       "      <td>t3_gaszeo</td>\n",
       "      <td>False</td>\n",
       "      <td>[NOUN, VERB, PROPN, PUNCT, SCONJ, NOUN, AUX, A...</td>\n",
       "      <td>[NN, VB, NNP, ,, IN, NN, VBZ, JJ, ,, PRP, MD, ...</td>\n",
       "      <td>[nsubj, ccomp, dobj, punct, mark, nsubj, advcl...</td>\n",
       "      <td>[X'xxx, xxx, xxxx, ,, xx, xxxx, xx, xxx, ,, xx...</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.0534</td>\n",
       "      <td>0.038294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>originalmuggins</td>\n",
       "      <td>Lol Apple with the slow roll. You gotta respec...</td>\n",
       "      <td>[lol, apple, slow, gotta, respect]</td>\n",
       "      <td>2020-04-30 20:11:46</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_gaszeo</td>\n",
       "      <td>False</td>\n",
       "      <td>[PROPN, PROPN, ADP, DET, ADJ, NOUN, PUNCT, PRO...</td>\n",
       "      <td>[NNP, NNP, IN, DT, JJ, NN, ., PRP, VBD, TO, VB...</td>\n",
       "      <td>[compound, ROOT, prep, det, amod, pobj, punct,...</td>\n",
       "      <td>[Xxx, Xxxxx, xxxx, xxx, xxxx, xxxx, ., Xxx, xx...</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>ch1p_skylark</td>\n",
       "      <td>AAPL at 4:30, UAL 4:15 EST</td>\n",
       "      <td>[aapl, ual, est]</td>\n",
       "      <td>2020-04-30 20:11:42</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_gaszeo</td>\n",
       "      <td>False</td>\n",
       "      <td>[VERB, ADP, NUM, PUNCT, PROPN, NUM, NOUN]</td>\n",
       "      <td>[VBD, IN, CD, ,, NNP, CD, NN]</td>\n",
       "      <td>[ROOT, prep, pobj, punct, npadvmod, nummod, ap...</td>\n",
       "      <td>[XXXX, xx, d:dd, ,, XXX, d:dd, XXX]</td>\n",
       "      <td>0.7741</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>facehuggerpoop</td>\n",
       "      <td>bought one apple call before close.\\n\\nI think...</td>\n",
       "      <td>[bought, one, apple, call, think, going, lose,...</td>\n",
       "      <td>2020-04-30 20:11:08</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_gaszeo</td>\n",
       "      <td>False</td>\n",
       "      <td>[VERB, NUM, NOUN, NOUN, ADP, NOUN, PUNCT, SPAC...</td>\n",
       "      <td>[VBD, CD, NN, NN, IN, NN, ., _SP, PRP, VBP, PR...</td>\n",
       "      <td>[ROOT, nummod, compound, dobj, advmod, advmod,...</td>\n",
       "      <td>[xxxx, xxx, xxxx, xxxx, xxxx, xxxx, ., \\n\\n, X...</td>\n",
       "      <td>-0.9975</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                                raw  \\\n",
       "880             LegNest                               oof aapl  **109.26**   \n",
       "532            Agent248  Holding apple calls is making me gay from last...   \n",
       "568            sandawg_  Hey guys! 😃 AAPL gang checking in 🙂. Today was...   \n",
       "457            Graxiano  AAPL is so last week, hype from split has died...   \n",
       "498    Throwawayeconboi  AAPL is as FLAT as a white woman’s ass. \\n\\n\\n...   \n",
       "387  strawberry-jam-boy  Spce tsla se lulu plug wmt upwk amd aapl msft ...   \n",
       "107      philmacrack123  Y'all see amazon, if apple is bad, we will sel...   \n",
       "112     originalmuggins  Lol Apple with the slow roll. You gotta respec...   \n",
       "121        ch1p_skylark                         AAPL at 4:30, UAL 4:15 EST   \n",
       "196      facehuggerpoop  bought one apple call before close.\\n\\nI think...   \n",
       "\n",
       "                                                  body          created_utc  \\\n",
       "880                                        [oof, aapl]  2020-10-30 08:06:39   \n",
       "532  [holding, apple, call, making, gay, last, 2, day]  2020-09-23 20:11:29   \n",
       "568  [hey, aapl, gang, checking, today, wa, tough, ...  2020-09-23 20:11:05   \n",
       "457     [aapl, last, hype, split, ha, died, tsla, new]  2020-08-18 08:06:48   \n",
       "498           [aapl, flat, white, fuuuuuuuuuuuuuuuuuu]  2020-08-18 08:03:50   \n",
       "387  [spce, tsla, se, lulu, plug, wmt, upwk, amd, a...  2020-07-12 20:09:20   \n",
       "107  [see, apple, gonna, need, appl, easily, carry,...  2020-04-30 20:11:50   \n",
       "112                 [lol, apple, slow, gotta, respect]  2020-04-30 20:11:46   \n",
       "121                                   [aapl, ual, est]  2020-04-30 20:11:42   \n",
       "196  [bought, one, apple, call, think, going, lose,...  2020-04-30 20:11:08   \n",
       "\n",
       "     score    link_id  is_submitter  \\\n",
       "880      1  t3_jkhkw8         False   \n",
       "532      2  t3_iyhxsc         False   \n",
       "568     -1  t3_iyhxsc         False   \n",
       "457      3  t3_iblrnn         False   \n",
       "498      6  t3_iblrnn         False   \n",
       "387      6  t3_hq11ao         False   \n",
       "107      6  t3_gaszeo         False   \n",
       "112      2  t3_gaszeo         False   \n",
       "121      1  t3_gaszeo         False   \n",
       "196      1  t3_gaszeo         False   \n",
       "\n",
       "                                                   Pos  \\\n",
       "880  [PROPN, PROPN, SPACE, PUNCT, PUNCT, NUM, PUNCT...   \n",
       "532  [VERB, NOUN, NOUN, AUX, VERB, PRON, ADJ, ADP, ...   \n",
       "568  [INTJ, NOUN, PUNCT, PROPN, PROPN, NOUN, VERB, ...   \n",
       "457  [PROPN, AUX, ADV, ADJ, NOUN, PUNCT, NOUN, ADP,...   \n",
       "498  [PROPN, AUX, ADV, ADJ, SCONJ, DET, ADJ, NOUN, ...   \n",
       "387  [NOUN, NOUN, VERB, NOUN, PROPN, PROPN, PROPN, ...   \n",
       "107  [NOUN, VERB, PROPN, PUNCT, SCONJ, NOUN, AUX, A...   \n",
       "112  [PROPN, PROPN, ADP, DET, ADJ, NOUN, PUNCT, PRO...   \n",
       "121          [VERB, ADP, NUM, PUNCT, PROPN, NUM, NOUN]   \n",
       "196  [VERB, NUM, NOUN, NOUN, ADP, NOUN, PUNCT, SPAC...   \n",
       "\n",
       "                                                   Tag  \\\n",
       "880            [NNP, NNP, _SP, NFP, NFP, CD, NFP, NFP]   \n",
       "532  [VBG, NN, NNS, VBZ, VBG, PRP, JJ, IN, JJ, CD, ...   \n",
       "568  [UH, NNS, ., NNP, NNP, NN, VBG, IN, NNP, ., NN...   \n",
       "457  [NNP, VBZ, RB, JJ, NN, ,, NN, IN, NN, VBZ, VBN...   \n",
       "498  [NNP, VBZ, RB, JJ, IN, DT, JJ, NN, POS, NN, .,...   \n",
       "387  [NN, NNS, VBP, NN, NNP, NNP, NNP, NNP, NNP, NN...   \n",
       "107  [NN, VB, NNP, ,, IN, NN, VBZ, JJ, ,, PRP, MD, ...   \n",
       "112  [NNP, NNP, IN, DT, JJ, NN, ., PRP, VBD, TO, VB...   \n",
       "121                      [VBD, IN, CD, ,, NNP, CD, NN]   \n",
       "196  [VBD, CD, NN, NN, IN, NN, ., _SP, PRP, VBP, PR...   \n",
       "\n",
       "                                                   Dep  \\\n",
       "880          [amod, ROOT, , dep, dep, ROOT, dep, ROOT]   \n",
       "532  [csubj, compound, dobj, aux, ROOT, nsubj, ccom...   \n",
       "568  [intj, ROOT, punct, compound, compound, nsubj,...   \n",
       "457  [nsubj, ccomp, advmod, amod, npadvmod, punct, ...   \n",
       "498  [nsubj, ROOT, advmod, acomp, prep, det, amod, ...   \n",
       "387  [compound, compound, compound, compound, compo...   \n",
       "107  [nsubj, ccomp, dobj, punct, mark, nsubj, advcl...   \n",
       "112  [compound, ROOT, prep, det, amod, pobj, punct,...   \n",
       "121  [ROOT, prep, pobj, punct, npadvmod, nummod, ap...   \n",
       "196  [ROOT, nummod, compound, dobj, advmod, advmod,...   \n",
       "\n",
       "                                                 Shape   FLAIR   VADER  \\\n",
       "880                 [xxx, xxxx,  , *, *, ddd.dd, *, *] -0.9887  0.0000   \n",
       "532  [Xxxxx, xxxx, xxxx, xx, xxxx, xx, xxx, xxxx, x... -0.9977  0.0000   \n",
       "568  [Xxx, xxxx, !, 😃, XXXX, xxxx, xxxx, xx, 🙂, ., ... -0.9709  0.6430   \n",
       "457  [XXXX, xx, xx, xxxx, xxxx, ,, xxxx, xxxx, xxxx... -0.9999 -0.5574   \n",
       "498  [XXXX, xx, xx, XXXX, xx, x, xxxx, xxxx, ’x, xx... -0.9963 -0.5423   \n",
       "387  [Xxxx, xxxx, xx, xxxx, xxxx, xxx, xxxx, xxx, x...  0.9968  0.3612   \n",
       "107  [X'xxx, xxx, xxxx, ,, xx, xxxx, xx, xxx, ,, xx... -0.9823 -0.0534   \n",
       "112  [Xxx, Xxxxx, xxxx, xxx, xxxx, xxxx, ., Xxx, xx...  0.9679  0.7096   \n",
       "121                [XXXX, xx, d:dd, ,, XXX, d:dd, XXX]  0.7741  0.0000   \n",
       "196  [xxxx, xxx, xxxx, xxxx, xxxx, xxxx, ., \\n\\n, X... -0.9975 -0.4019   \n",
       "\n",
       "         BLOB  \n",
       "880  0.000000  \n",
       "532  0.208333  \n",
       "568  0.136508  \n",
       "457 -0.006397  \n",
       "498 -0.012500  \n",
       "387  0.000000  \n",
       "107  0.038294  \n",
       "112  0.250000  \n",
       "121  0.000000  \n",
       "196  0.285714  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By frequency appareance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Timedelta\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-f1330f410075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticker_posts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#5days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnext_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWSB_preprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWSB_preprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetween\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclusive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"Timedelta\") to str"
     ]
    }
   ],
   "source": [
    "# K: Not sure what the issue is: TypeError: can only concatenate str (not \"Timedelta\") to str\n",
    "\n",
    "ticker_cluster =[]\n",
    "\n",
    "for date in ticker_posts['created_utc']:\n",
    "    for step in range(7200): #5days\n",
    "        next_date = str(date) + pd.Timedelta(step, unit='m')\n",
    "    \n",
    "        a = WSB_preprocessed_data[WSB_preprocessed_data['created_utc'].between(date, next_date, inclusive=True)]\n",
    "        b = ticker_posts[ticker_posts['created_utc'].between(date, next_date, inclusive=True)]  \n",
    "    \n",
    "        if a * 0.9 > b:\n",
    "            ticker_cluster.append([date, next_date])\n",
    "            ticker_posts = ticker_posts[ticker_posts['created_utc']>next_date]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_cluster =[]\n",
    "\n",
    "for post in ticker_posts['link_id'].unique():\n",
    "    if len(ticker_posts[ticker_posts['link_id']==post]) >= len(WSB_preprocessed_data[WSB_preprocessed_data['link_id']==post]) * 0.7:\n",
    "        ticker_cluster.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Ticker Data \n",
    "- Ticker\n",
    "- Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not needed? I already added the data at the top, scraped from InteractiveBrokers\n",
    "# import yfinance as yf\n",
    "# data = yf.download(ticker, interval=frequency, start=\"2012-01-31\", end=\"2021-02-16\")\n",
    "\n",
    "# df_train, df_test = train_test_split(\n",
    "#   df,\n",
    "#   test_size=0.1,\n",
    "#   random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Need to time series split to preserve time series\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=0.8, n_splits=3, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_tcn, build_lstm\n",
    "\n",
    "class PerformTraining(PreProcessing):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class performs the training of the desired model\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    seed : int\n",
    "        the integer of the seed utilised for reproducibility \n",
    "    DATA_DIR : str\n",
    "        a string indicating the directory containing the raw stock and WSB data\n",
    "    INTERM_DATA_DIR : str\n",
    "        a string indicating the directory containing intermediate computed data\n",
    "    MODEL_DIR : str\n",
    "        a string indicating the directory containing created models\n",
    "    preprocessing_params : dict\n",
    "        a dictionary of preprocessing parameters\n",
    "    saved_dataset_pkl : str\n",
    "        a string indicating the directory containing the previously preprocess-compiled dataset\n",
    "    saved_dataset_params_pkl : str\n",
    "        a string indicating the directory containing the previously saved variance thresholded parameters\n",
    "    data : (ndarray, ndarray, ndarray, ndarray, ndarray, ndarray)    \n",
    "        a tuple of either 6, if tuning is True, or 2 ndarrays,\n",
    "        (X_train, y_train, X_val, y_val, X_test, y_test) or (X_train, y_train)\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    reproducible_results()\n",
    "        Sets seed and ensures all deterministic operations are reproducible\n",
    "    retrieve_data()\n",
    "        Retrieves the data given the data directory and folders\n",
    "    prepare_data(preprocessing_params, tuning=True):\n",
    "        Combines the preprocessing methods and splits the data for training \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, DATA_DIR, INTERM_DATA_DIR, MODEL_DIR, DATA_SRCS, preprocessing_params, model_type, tuning=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        DATA_DIR : str\n",
    "            a string indicating the directory containing the raw stock and WSB data\n",
    "        INTERM_DATA_DIR : str\n",
    "            a string indicating the directory containing intermediate computed data\n",
    "        MODEL_DIR : str\n",
    "            a string indicating the directory containing created models\n",
    "        preprocessing_params : dict\n",
    "            a dictionary containing the following keys, \n",
    "                ... to fill\n",
    "        data_aug_params : dict\n",
    "            a dictionary containing the following keys, \n",
    "                ... to fill\n",
    "        tuning : bool\n",
    "            a boolean indicating whether this is for tuning or the final model\n",
    "            for tuning, will prepare a threeway train, validation test split\n",
    "            if false, will not split the data\n",
    "        \"\"\"\n",
    "\n",
    "        self.seed = 7\n",
    "\n",
    "        self.DATA_DIR = DATA_DIR\n",
    "        self.INTERM_DATA_DIR = INTERM_DATA_DIR\n",
    "        self.MODEL_DIR = MODEL_DIR\n",
    "        self.DATA_SRCS = DATA_SRCS\n",
    "        self.preprocessing_params = preprocessing_params\n",
    "        self.tuning = tuning\n",
    "        self.max_epochs = 100\n",
    "        \n",
    "        # Define pickle file where compiled dataset is saved\n",
    "        # This is used to reduce unnecessary recomputations of the same preprocessing \n",
    "        self.saved_dataset_pkl = os.path.join(self.INTERM_DATA_DIR, 'dataset_var_thresh_preprocessed.pkl')\n",
    "        self.saved_dataset_params_pkl = os.path.join(self.INTERM_DATA_DIR, 'dataset_params.pkl')\n",
    "        \n",
    "        self.data = self.prepare_data()\n",
    "        \n",
    "        if model_type == 'tcn':\n",
    "            perform_cv_gs_training(build_tcn, ...)\n",
    "            \n",
    "        if model_type == 'lstm':\n",
    "            perform_cv_gs_training(build_lstm, ...)\n",
    "\n",
    "        def reproducible_results(self):\n",
    "        \n",
    "            \"\"\"Obtain reproducible results with keras, source: https://stackoverflow.com/a/52897216\"\"\"\n",
    "\n",
    "            # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "            os.environ['PYTHONHASHSEED'] = str(self.seed)\n",
    "\n",
    "            # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "            random.seed(self.seed)\n",
    "\n",
    "            # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "            # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "            tf.compat.v1.set_random_seed(self.seed)\n",
    "\n",
    "            # 5. Configure a new global `tensorflow` session\n",
    "            session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "            sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "            K.set_session(sess)\n",
    "\n",
    "        def prepare_data(self):\n",
    "            \n",
    "            \"\"\"Readies the data for training by splitting into training, validation and testing sets and launching the preprocessing\n",
    "            \n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        def perform_cv_gs_training(self, model_fn, checkpoint_filepath, flush, save_plot):\n",
    "\n",
    "            \"\"\"Performs cross-validated grid search training for selected model function\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            model_fn : function\n",
    "                a function which creates a keras compiled model\n",
    "            checkpoint_filepath : str\n",
    "                a string indicating where to save the plots and grid search results \n",
    "                of the cross-validated grid search\n",
    "            flush : bool\n",
    "                a boolean indicating whether to flush the indicated checkpoint_filepath\n",
    "            save_plot : bool\n",
    "                a boolean indicating whether to save the plots depicting performance over the folds\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            pandas.DataFrame\n",
    "                a pandas dataframe containing the results of the grid search, \n",
    "                specifically average performance for given hyperparameters\n",
    "            \"\"\"  \n",
    "\n",
    "            pkl_name = os.path.join(checkpoint_filepath, 'gs_res.pkl')\n",
    "            if os.path.isfile(pkl_name) and flush==False:\n",
    "                with open(pkl_name, 'rb') as f:\n",
    "                    gs_res = pkl.load(f)\n",
    "            else: \n",
    "                gs_res = []\n",
    "\n",
    "            X_train, y_train, X_val, y_val, X_test, y_test = self.data\n",
    "\n",
    "            # Merge inputs and targets for K-fold cross validation\n",
    "            inputs = np.concatenate((X_train, X_val), axis=0)\n",
    "            targets = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "            for idx, params in enumerate(self.model_gs_params): \n",
    "\n",
    "                print(\"=================================================\")\n",
    "                print(\"Presenting Results for: %s/%s Hyperparameter Combination\" % (idx+1, len(self.model_gs_params)))\n",
    "\n",
    "                model_params = self.model_param_setup(params)\n",
    "                print(model_params)\n",
    "\n",
    "                batch_size = model_params['batch_size']\n",
    "\n",
    "                # Create backlog for accuracy in each fold\n",
    "                val_fold_accuracy = []\n",
    "                test_fold_accuracy = []\n",
    "\n",
    "                # Define the K-fold Cross Validator\n",
    "                kfold = model_selection.KFold(n_splits=self.k_folds, shuffle=True)\n",
    "\n",
    "                # K-fold Cross Validation model evaluation\n",
    "                fold_no = 1\n",
    "                for train, val in kfold.split(inputs, targets):\n",
    "\n",
    "                    try:     \n",
    "                        # Increase fold number\n",
    "                        print(\"Performing fold: %s/%s\" % (fold_no, self.k_folds))\n",
    "                        fold_no = fold_no + 1\n",
    "\n",
    "                        X_iter_train, X_iter_val = inputs[train], inputs[val]\n",
    "                        y_iter_train, y_iter_val = targets[train], targets[val]\n",
    "\n",
    "                        # Prepare the training dataset\n",
    "                        train_dataset = tf.data.Dataset.from_tensor_slices((X_iter_train, y_iter_train))\n",
    "                        train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "                        # Prepare the validation dataset\n",
    "                        val_dataset = tf.data.Dataset.from_tensor_slices((X_iter_val, y_iter_val))\n",
    "                        val_dataset = val_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "                        model = model_fn(X_train, **model_params)\n",
    "\n",
    "                        # Create Tensorboard\n",
    "                        logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch', profile_batch=0)\n",
    "                        # Model Checkpoint Callback\n",
    "                        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(checkpoint_filepath,'checkpoint'), save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True)\n",
    "                        # Early Stopping Callback\n",
    "                        early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 2)\n",
    "\n",
    "                        # Train the model\n",
    "                        training_history = model.fit(X_iter_train, y_iter_train, batch_size=batch_size, validation_data=(X_iter_val, y_iter_val), \n",
    "                                                     callbacks = [tensorboard_callback,\n",
    "                                                                  early_stopping_callback,\n",
    "                                                                  checkpoint_callback],\n",
    "                                                     epochs=self.max_epochs, verbose=1)\n",
    "\n",
    "            return \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
