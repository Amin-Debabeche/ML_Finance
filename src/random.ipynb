{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Garbages\n",
    "import wrds\n",
    "db=wrds.Connection(wrds_username='debabech', wrds_username='Electro1004$')\n",
    "db.create_pgpass_file()\n",
    "permcos = db.get_table(library='crsp', table='stocknames')[[\"permco\",\"ticker\"]]\n",
    "ticker = asset\n",
    "permco = permcos[permcos.ticker == ticker].permco.values[0]\n",
    "\n",
    "req = f\"select prc, date from crsp.dsf where permco in ({permco}) and date >='2012-01-31' and date <='2021-02-16'\"\n",
    "asset = db.raw_sql(req, date_cols=['date'])\n",
    "\n",
    "asset = asset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextSentiment(Base):\n",
    "    \"\"\"Predict fine-grained sentiment scores using FastText\"\"\"\n",
    "    def __init__(self, model_file: str=None) -> None:\n",
    "        super().__init__()\n",
    "        import fasttext\n",
    "        self.model = fasttext.load_model(model_file)\n",
    "\n",
    "    def score(self, text: str) -> int:\n",
    "        # Predict just the top label (hence 1 index below)\n",
    "        labels, probabilities = self.model.predict(text, 1)\n",
    "        pred = int(labels[0][-1])\n",
    "        return pred\n",
    "\n",
    "    def predict(self, train_file: None, test_file: str, lower_case: bool) -> pd.DataFrame:\n",
    "        df = self.read_data(test_file, lower_case)\n",
    "        df['pred'] = df['text'].apply(self.score)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased' # since “BAD” might convey more sentiment than “bad” on a forum\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "tokens = tokenizer.tokenize(XXX)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "    XXX,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors)\n",
    "\n",
    "encoding.keys()\n",
    "\n",
    "dict_keys(['input_ids', 'attention_mask'])    \n",
    "\n",
    "token_lens = []\n",
    "\n",
    "for txt in df.content:\n",
    "  tokens = tokenizer.encode(txt, max_length=512)\n",
    "  token_lens.append(len(tokens))\n",
    "  \n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256])\n",
    "plt.xlabel('Token count')\n",
    "\n",
    "MAX_LEN = XXX #See plot to select sequence size, take a bit higher\n",
    "\n",
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "    \n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "    \n",
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.1,\n",
    "  random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_val, df_test = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5,\n",
    "  random_state=RANDOM_SEED\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "adaexam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
